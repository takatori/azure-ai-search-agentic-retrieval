{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "import datasets as ds\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’ç§’å˜ä½ã§è¨­å®šï¼ˆä¾‹: 100ç§’ï¼‰\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"36000\"  # ğŸ‘ˆ ã“ã®è¡Œã‚’è¿½åŠ \n",
    "\n",
    "# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š\n",
    "local_dir = Path(\"datasets/JDocQA\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# æ—¢ã«ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜æ¸ˆã¿ãªã‚‰ãã‚Œã‚’èª­ã¿è¾¼ã‚€\n",
    "if (local_dir / \"dataset_info.json\").exists():\n",
    "    print(\"ğŸ” Loading dataset from local disk...\")\n",
    "    dataset = ds.load_from_disk(str(local_dir))\n",
    "else:\n",
    "    print(\"â¬‡ï¸ Downloading dataset from Hugging Face Hub...\")\n",
    "    dataset = ds.load_dataset(\n",
    "        path=\"shunk031/JDocQA\",\n",
    "        rename_pdf_category=True,\n",
    "        trust_remote_code=True,\n",
    "        storage_options={\n",
    "            \"client_kwargs\": {\"timeout\": aiohttp.ClientTimeout(total=36000)}\n",
    "        },\n",
    "    )\n",
    "    dataset.save_to_disk(str(local_dir))\n",
    "    print(f\"ğŸ’¾ Dataset saved locally to {local_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    IndexingParameters,\n",
    "    InputFieldMappingEntry,\n",
    "    LexicalAnalyzerName,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchField,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataSourceType,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "from tqdm import tqdm\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(\n",
    "    credential, \"https://search.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# ã©ã® split ã‚’ä½¿ã†ã‹\n",
    "SPLIT = \"test\"  # \"validation\" ã‚‚å¯\n",
    "MAX_SAMPLES = 300  # None ã§å…¨ä»¶\n",
    "CHUNK_SIZE = 700\n",
    "CHUNK_OVERLAP = 200\n",
    "USE_ORIGINAL = False\n",
    "\n",
    "SEARCH_ENDPOINT = os.getenv(\"SEARCH_ENDPOINT\")\n",
    "AOAI_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AOAI_EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "AOAI_EMBEDDING_DEPLOYMENT = \"text-embedding-3-large\"\n",
    "AOAI_GPT_MODEL = \"gpt-5-mini\"\n",
    "AOAI_GPT_DEPLOYMENT = \"gpt-5-mini\"\n",
    "INDEX_NAME = \"jdocqa-index\"\n",
    "KNOWLEDGE_SOURCE_NAME = \"jdocqa-knowledge-source\"\n",
    "KNOWLEDGE_AGENT_NAME = \"jdocqa-knowledge-agent\"\n",
    "SEARCH_API_VERSION = \"2025-08-01-preview\"\n",
    "DATA_SOURCE_NAME = \"ds-jdocqa-chunks\"\n",
    "SKILLSET_NAME = \"ss-jdocqa-embed\"\n",
    "INDEXER_NAME = \"idx-jdocqa\"\n",
    "DIM = 3072\n",
    "BLOB_CONTAINER = \"jdocqa-chunks\"\n",
    "BLOB_PREFIX = \"docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = dataset[SPLIT]\n",
    "\n",
    "# if MAX_SAMPLES is not None:\n",
    "#     data = data.select(range(min(len(data), MAX_SAMPLES)))\n",
    "\n",
    "# # åŒä¸€contextã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ã—ã¦doc_idã‚’å‰²ã‚Šå½“ã¦\n",
    "# contexts = [ex[\"context\"] for ex in data]\n",
    "# unique_contexts, inverse_indices = np.unique(contexts, return_inverse=True)\n",
    "# # unique_contexts -> [\"Xã®æœ¬æ–‡\", \"Yã®æœ¬æ–‡\"]  â€»è¾æ›¸é †\n",
    "# # inverse_indices -> [0, 1, 0]              # å…ƒã®0ç•ª/2ç•ªã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯é…åˆ—ã®0ç•ªã«å¯¾å¿œ\n",
    "\n",
    "# doc_ids = np.arange(len(unique_contexts))\n",
    "\n",
    "# queries = [ex[\"question\"] for ex in data]\n",
    "# gold_doc_ids = inverse_indices\n",
    "\n",
    "\n",
    "# def mk_doc(i: int) -> dict:\n",
    "#     return {\n",
    "#         \"id\": str(i),\n",
    "#         \"content\": unique_contexts[i],\n",
    "#         \"pdf_category\": str(data[i][\"pdf_category\"])\n",
    "#         if i < len(data) and \"pdf_category\" in data[i]\n",
    "#         else \"N/A\",\n",
    "#     }\n",
    "\n",
    "\n",
    "# docs = [mk_doc(i) for i in range(len(unique_contexts))]\n",
    "\n",
    "# print(f\"#docs={len(docs)}  #queries={len(queries)}  (split={SPLIT})\")\n",
    "# print(\"Example doc:\", docs[155] if docs else None)\n",
    "# print(\n",
    "#     \"Example QA:\",\n",
    "#     {\"q\": queries[0], \"gold_doc_id\": int(gold_doc_ids[0])} if queries else None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split_ja(text: str):\n",
    "    t = re.sub(r\"[\\r\\n]+\", \"ã€‚\", text)\n",
    "    t = re.sub(r\"ã€‚+\", \"ã€‚\", t)\n",
    "    parts = [p.strip() for p in t.split(\"ã€‚\") if p.strip()]\n",
    "    return [p + \"ã€‚\" for p in parts]\n",
    "\n",
    "\n",
    "def make_chunks(text: str, size=700, overlap=200):\n",
    "    sents = sentence_split_ja(text)\n",
    "    chunks, buf, cur = [], [], 0\n",
    "    for s in sents:\n",
    "        if cur + len(s) > size and buf:\n",
    "            chunks.append(\"\".join(buf))\n",
    "            keep = \"\".join(buf)[-overlap:] if overlap > 0 else \"\"\n",
    "            buf, cur = ([keep] if keep else []), len(keep)\n",
    "        buf.append(s)\n",
    "        cur += len(s)\n",
    "    if buf:\n",
    "        chunks.append(\"\".join(buf))\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= size * 2:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            step = max(1, size - overlap)\n",
    "            for i in range(0, len(c), step):\n",
    "                final.append(c[i : i + size])\n",
    "    return final\n",
    "\n",
    "\n",
    "def stable_id(text: str) -> str:\n",
    "    import hashlib\n",
    "\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[SPLIT]\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    data = data.select(range(min(MAX_SAMPLES, len(data))))\n",
    "\n",
    "\n",
    "def pick_ctx(ex):\n",
    "    return (\n",
    "        (ex.get(\"original_context\") or ex.get(\"context\"))\n",
    "        if USE_ORIGINAL\n",
    "        else ex.get(\"context\")\n",
    "    )\n",
    "\n",
    "\n",
    "chunk_docs = {}\n",
    "parent_to_chunks = {}\n",
    "qa_gold_sets = []\n",
    "queries = []\n",
    "\n",
    "for ex in tqdm(data):\n",
    "    ctx = pick_ctx(ex) or \"\"\n",
    "    parent_id = stable_id(ctx) if ctx else stable_id(\"EMPTY\")\n",
    "    chs = make_chunks(ctx, CHUNK_SIZE, CHUNK_OVERLAP) if ctx else [\" \"]\n",
    "    ids = []\n",
    "    for ch in chs:\n",
    "        cid = stable_id(parent_id + \"|\" + ch)\n",
    "        if cid not in chunk_docs:\n",
    "            chunk_docs[cid] = {\n",
    "                \"id\": cid,\n",
    "                \"parent_id\": parent_id,\n",
    "                \"content\": ch,\n",
    "                # \"category\": str(ex.get(\"category\",\"unknown\"))\n",
    "            }\n",
    "        ids.append(cid)\n",
    "    parent_to_chunks[parent_id] = ids\n",
    "    qa_gold_sets.append(set(ids))\n",
    "    queries.append(ex.get(\"question\", \"\"))\n",
    "\n",
    "docs = list(chunk_docs.values())\n",
    "print(f\"parents={len(parent_to_chunks)} chunks={len(docs)} queries={len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    index_client.delete_index(INDEX_NAME)\n",
    "except Exception:\n",
    "    pass  # å­˜åœ¨ã—ãªã„å ´åˆã¯ç„¡è¦–\n",
    "\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=[\n",
    "        SearchField(\n",
    "            name=\"id\",\n",
    "            type=\"Edm.String\",\n",
    "            key=True,\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"parent_id\",\n",
    "            type=\"Edm.String\",\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"content\",\n",
    "            type=\"Edm.String\",\n",
    "            searchable=True,\n",
    "            analyzer_name=LexicalAnalyzerName.JA_LUCENE,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"page_embedding_text_3_large\",\n",
    "            type=\"Collection(Edm.Single)\",\n",
    "            stored=True,\n",
    "            retrievable=True,\n",
    "            vector_search_dimensions=DIM,\n",
    "            vector_search_profile_name=\"hnsw_text_3_large\",\n",
    "        ),\n",
    "        # SearchField(name=\"page_chunk\", type=\"Edm.String\", filterable=False, sortable=False, facetable=False),\n",
    "        #\n",
    "        # SearchField(name=\"page_number\", type=\"Edm.Int32\", filterable=True, sortable=True, facetable=True)\n",
    "    ],\n",
    "    vector_search=VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"hnsw_text_3_large\",\n",
    "                algorithm_configuration_name=\"alg\",\n",
    "                vectorizer_name=\"azure_openai_text_3_large\",\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"alg\")],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"azure_openai_text_3_large\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=AOAI_ENDPOINT,\n",
    "                    deployment_name=AOAI_EMBEDDING_DEPLOYMENT,\n",
    "                    model_name=AOAI_EMBEDDING_MODEL,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    "    semantic_search=SemanticSearch(\n",
    "        default_configuration_name=\"semantic_config\",\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"semantic_config\",\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    content_fields=[SemanticField(field_name=\"content\")]\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=SEARCH_ENDPOINT, credential=credential)\n",
    "index_client.create_or_update_index(index)\n",
    "print(f\"Index '{INDEX_NAME}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Uploading {len(docs)} documents to index '{index_name}'...\")\n",
    "# print(f\"Sample document: {docs[0] if docs else 'N/A'}\")\n",
    "\n",
    "# with SearchIndexingBufferedSender(endpoint=SEARCH_ENDPOINT, index_name=index_name, credential=credential) as client:\n",
    "#     client.upload_documents(documents=docs)\n",
    "\n",
    "# print(f\"Documents uploaded to index '{index_name}' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "container_client = bsc.get_container_client(BLOB_CONTAINER)\n",
    "try:\n",
    "    container_client.create_container()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def upload_docs_to_blob(docs, prefix=BLOB_PREFIX):\n",
    "    for d in tqdm(docs):\n",
    "        name = f\"{prefix}/{d['id']}.json\"\n",
    "        data = json.dumps(d, ensure_ascii=False).encode(\"utf-8\")\n",
    "        container_client.upload_blob(\n",
    "            name,\n",
    "            data,\n",
    "            overwrite=True,\n",
    "            content_settings=ContentSettings(content_type=\"application/json\"),\n",
    "        )\n",
    "    return len(docs)\n",
    "\n",
    "\n",
    "print(\"Uploaded:\", upload_docs_to_blob(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = SearchIndexerDataContainer(\n",
    "    name=BLOB_CONTAINER,\n",
    "    query=BLOB_PREFIX,  # \"docs\" ä»¥ä¸‹ã ã‘å–ã‚Šè¾¼ã‚€ã€‚ã‚³ãƒ³ãƒ†ãƒŠå…¨ä½“ãªã‚‰ None\n",
    ")\n",
    "\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "    name=DATA_SOURCE_NAME,\n",
    "    type=SearchIndexerDataSourceType.AZURE_BLOB,  # ä»–: AZURE_TABLE, AZURE_SQL, COSMOSDB, ADLSGEN2 ãªã©\n",
    "    connection_string=AZURE_STORAGE_CONNECTION_STRING,\n",
    "    container=container,\n",
    "    description=\"JDocQA chunk JSONs in Blob Storage\",\n",
    ")\n",
    "\n",
    "# æ—¢å­˜ãŒã‚ã‚Œã°æ›´æ–°ã€ãªã‘ã‚Œã°ä½œæˆ\n",
    "indexer_client = SearchIndexerClient(endpoint=SEARCH_ENDPOINT, credential=credential)\n",
    "indexer_client.create_or_update_data_source_connection(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in indexer_client.get_data_source_connections():\n",
    "    print(source.name, source.type)\n",
    "\n",
    "print(f\"Data Source '{DATA_SOURCE_NAME}' created or updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    indexer_client.delete_skillset(SKILLSET_NAME)\n",
    "except Exception:\n",
    "    pass  # å­˜åœ¨ã—ãªã„å ´åˆã¯ç„¡è¦–\n",
    "\n",
    "# ===== Skillset ä½œæˆ =====\n",
    "# AzureOpenAI Embedding Skill ã‚’ \"æ±ç”¨ã‚¹ã‚­ãƒ«\" ã¨ã—ã¦ä½œæˆã—ã€è¿½åŠ ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã§å¿…è¦é …ç›®ã‚’æ¸¡ã™\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(\n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",\n",
    "    context=\"/document\",\n",
    "    resource_url=AOAI_ENDPOINT,\n",
    "    deployment_name=AOAI_EMBEDDING_DEPLOYMENT,\n",
    "    model_name=AOAI_EMBEDDING_MODEL,\n",
    "    dimensions=DIM,\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),\n",
    "    ],\n",
    "    outputs=[OutputFieldMappingEntry(name=\"embedding\", target_name=\"emb\")],\n",
    ")\n",
    "\n",
    "skillset = SearchIndexerSkillset(\n",
    "    name=SKILLSET_NAME,\n",
    "    skills=[embedding_skill],\n",
    "    description=\"JDocQA index-time embedding skillset\",\n",
    ")\n",
    "\n",
    "# æ—¢å­˜ãŒã‚ã‚Œã°æ›´æ–°ã€ãªã‘ã‚Œã°ä½œæˆ\n",
    "indexer_client.create_or_update_skillset(skillset)\n",
    "\n",
    "print(\"Skillset created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    indexer_client.delete_indexer(INDEXER_NAME)\n",
    "    print(f\"Deleted indexer: {INDEXER_NAME}\")\n",
    "except HttpResponseError as e:\n",
    "    # 404 ç›¸å½“ã¯ç„¡è¦–ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰\n",
    "    if getattr(e, \"status_code\", None) not in (404,):\n",
    "        raise\n",
    "\n",
    "indexer = SearchIndexer(\n",
    "    name=INDEXER_NAME,\n",
    "    data_source_name=DATA_SOURCE_NAME,\n",
    "    target_index_name=INDEX_NAME,\n",
    "    skillset_name=SKILLSET_NAME,  # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ç´ã¥ã‘\n",
    "    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ -> ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"id\", target_field_name=\"id\"),\n",
    "        FieldMapping(source_field_name=\"parent_id\", target_field_name=\"parent_id\"),\n",
    "        FieldMapping(source_field_name=\"content\", target_field_name=\"content\"),\n",
    "    ],\n",
    "    # ã‚¹ã‚­ãƒ«å‡ºåŠ› -> ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    # ä¾‹ï¼šEmbeddingSkill ã®å‡ºåŠ› \"/document/emb\" ã‚’ vector ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ \"content_vector\" ã¸\n",
    "    output_field_mappings=[\n",
    "        FieldMapping(\n",
    "            source_field_name=\"/document/emb\",\n",
    "            target_field_name=\"page_embedding_text_3_large\",\n",
    "        ),\n",
    "    ],\n",
    "    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    parameters=IndexingParameters(\n",
    "        configuration={\n",
    "            \"parsingMode\": \"json\",  # 1 JSON = 1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
    "            \"failOnUnsupportedContentType\": False,  # æœªå¯¾å¿œMIMEã§å¤±æ•—ã•ã›ãªã„\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "indexer_client.create_or_update_indexer(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Indexer ã‚’æ‰‹å‹•å®Ÿè¡Œ\n",
    "try:\n",
    "    indexer_client.reset_indexer(\n",
    "        INDEXER_NAME\n",
    "    )  # 1) å¤‰æ›´è¿½è·¡ï¼ˆãƒã‚¤ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯ï¼‰ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "    indexer_client.run_indexer(INDEXER_NAME)\n",
    "    print(f\"Run requested: {INDEXER_NAME}\")\n",
    "except HttpResponseError as e:\n",
    "    print(\"Run failed:\", e)\n",
    "    raise\n",
    "\n",
    "# 2) ç°¡æ˜“ãƒãƒ¼ãƒªãƒ³ã‚°ï¼ˆçŠ¶æ…‹ãŒ terminal ã«ãªã‚‹ã¾ã§å¾…ã¤ï¼‰\n",
    "terminal = {\"success\", \"transientFailure\", \"persistentFailure\", \"reset\"}\n",
    "for i in range(60):  # æœ€å¤§ ~5åˆ†å¾…æ©Ÿï¼ˆ5ç§’Ã—60ï¼‰\n",
    "    st = indexer_client.get_indexer_status(INDEXER_NAME)\n",
    "    last = st.last_result\n",
    "    status = getattr(last, \"status\", None)\n",
    "    processed = getattr(last, \"items_processed\", None)\n",
    "    failed = getattr(last, \"items_failed\", None)\n",
    "    print(f\"[{i}] status={status} processed={processed} failed={failed}\")\n",
    "\n",
    "    if status in terminal:\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "# 3) çµæœãƒã‚§ãƒƒã‚¯\n",
    "if status != \"success\":\n",
    "    raise RuntimeError(\n",
    "        f\"Indexer did not succeed. status={status}, processed={processed}, failed={failed}\"\n",
    "    )\n",
    "print(\"Indexer run completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=credential,\n",
    "    api_version=SEARCH_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"å®¶æ—æ‰‹å½“ã®æ”¯çµ¦æ¡ä»¶ã¯ï¼Ÿ\"\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    query_type=QueryType.SIMPLE,  # ã¾ãŸã¯ QueryType.SEMANTICï¼ˆsemanticè¨­å®šãŒã‚ã‚‹å ´åˆï¼‰\n",
    "    top=5,\n",
    "    select=[\"id\", \"parent_id\", \"content\", \"page_embedding_text_3_large\"],\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(\n",
    "        r[\"id\"],\n",
    "        r[\"@search.score\"],\n",
    "        r[\"content\"][:80],\n",
    "        r[\"page_embedding_text_3_large\"][:5],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorizableTextQuery(\n",
    "    text=\"å®¶æ—æ‰‹å½“ã®æ”¯çµ¦æ¡ä»¶ã¯ï¼Ÿ\",\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"page_embedding_text_3_large\",\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[vq], select=[\"id\", \"parent_id\", \"content\"]\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"id\"], r[\"@search.score\"], r[\"content\"][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"å®¶æ—æ‰‹å½“ã®æ”¯çµ¦æ¡ä»¶ã¯ï¼Ÿ\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query, k_nearest_neighbors=50, fields=\"page_embedding_text_3_large\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"id\", \"parent_id\", \"content\"],\n",
    "    top=5,\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"id\"], r[\"@search.score\"], r[\"content\"][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = SearchIndexKnowledgeSource(\n",
    "    name=KNOWLEDGE_SOURCE_NAME,\n",
    "    description=\"Knowledge source for Earth at night data\",\n",
    "    search_index_parameters=SearchIndexKnowledgeSourceParameters(\n",
    "        search_index_name=INDEX_NAME,\n",
    "        source_data_select=\"id,content,page_embedding_text_3_large\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "index_client.create_or_update_knowledge_source(\n",
    "    knowledge_source=ks, api_version=SEARCH_API_VERSION\n",
    ")\n",
    "print(f\"Knowledge source '{KNOWLEDGE_SOURCE_NAME}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_params = AzureOpenAIVectorizerParameters(\n",
    "    resource_url=AOAI_ENDPOINT,\n",
    "    deployment_name=AOAI_GPT_DEPLOYMENT,\n",
    "    model_name=AOAI_GPT_MODEL,\n",
    ")\n",
    "\n",
    "output_cfg = KnowledgeAgentOutputConfiguration(\n",
    "    modality=KnowledgeAgentOutputConfigurationModality.ANSWER_SYNTHESIS,\n",
    "    include_activity=True,\n",
    ")\n",
    "\n",
    "agent = KnowledgeAgent(\n",
    "    name=KNOWLEDGE_AGENT_NAME,\n",
    "    models=[KnowledgeAgentAzureOpenAIModel(azure_open_ai_parameters=aoai_params)],\n",
    "    knowledge_sources=[\n",
    "        KnowledgeSourceReference(\n",
    "            name=KNOWLEDGE_SOURCE_NAME,\n",
    "            reranker_threshold=2.5,\n",
    "        )\n",
    "    ],\n",
    "    output_configuration=output_cfg,\n",
    ")\n",
    "\n",
    "index_client.create_or_update_agent(agent, api_version=SEARCH_API_VERSION)\n",
    "print(f\"Knowledge agent '{KNOWLEDGE_AGENT_NAME}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "ã‚ãªãŸã¯ JDocQAï¼ˆæ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQAï¼‰ã®å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚\n",
    "ä¸ãˆã‚‰ã‚ŒãŸã€Œæ¤œç´¢æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ï¼ˆcontent, id, parent_id, page_number, category ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰ã€ã®ã¿ã‚’æ ¹æ‹ ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
    "æ ¹æ‹ ãŒè¶³ã‚Šãªã„å ´åˆã‚„çµè«–ã§ããªã„å ´åˆã¯ã€æ­£ç¢ºã«ã€ŒI don't knowã€ã¨ç­”ãˆã¾ã™ã€‚\n",
    "\n",
    "ã€å›ç­”åŸå‰‡ã€‘\n",
    "1) å‡ºå…¸åˆ¶ç´„: å›ç­”ã¯å¸¸ã«ä¸ãˆã‚‰ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã«é™å®šã€‚å¤–éƒ¨çŸ¥è­˜ã‚„æ¨æ¸¬ã§è£œã‚ãªã„ã€‚\n",
    "2) ç²’åº¦: è³ªå•ã®ç²’åº¦ã«åˆã‚ã›ã€éä¸è¶³ã®ãªã„è¦ç´„ã§ç­”ãˆã‚‹ã€‚å…·ä½“å€¤ã‚„å®šç¾©ã¯ãƒãƒ£ãƒ³ã‚¯ã«ã‚ã‚‹ã‚‚ã®ã®ã¿ä½¿ç”¨ã€‚\n",
    "3) çŸ›ç›¾å¯¾å¿œ: ãƒãƒ£ãƒ³ã‚¯é–“ã§å†…å®¹ãŒé£Ÿã„é•ã†å ´åˆã¯çŸ›ç›¾ã‚’æ˜ç¤ºã—ã€ã‚ˆã‚Šå…·ä½“ãƒ»æ–°ã—ã„ãƒ»æ³•ä»¤/è¦ç¨‹ãƒ™ãƒ¼ã‚¹ã®ã‚‚ã®ã‚’å„ªå…ˆã€‚\n",
    "4) ä¸ç¢ºå®Ÿæ€§: æƒ…å ±ãŒä¸ååˆ†/æ›–æ˜§ãªã‚‰ã€ŒI don't knowã€ã¾ãŸã¯æ¡ä»¶ã¤ãå›ç­”ï¼ˆæ ¹æ‹ ã‚’ç¤ºã™ï¼‰ã€‚\n",
    "5) å¼•ç”¨: å¯èƒ½ãªé™ã‚Šæ ¹æ‹ ãƒãƒ£ãƒ³ã‚¯ã® ID ã‚’ `[doc:{id}]` å½¢å¼ã§æœ«å°¾ã«æ·»ãˆã‚‹ï¼ˆè¤‡æ•°å¯ï¼‰ã€‚\n",
    "6) è¨€èª: å‡ºåŠ›ã¯è‡ªç„¶ãªæ—¥æœ¬èªã€‚ç®‡æ¡æ›¸ãã¯æœ€å¤§3â€“6é …ç›®ã«æŠ‘ãˆã‚‹ã€‚\n",
    "\n",
    "ã€ä½œæ¥­æ‰‹é †ï¼ˆå†…éƒ¨æ–¹é‡ï¼‰ã€‘\n",
    "- è³ªå•ã‚’åˆ†è§£ã—ã€å›ç­”ã«å¿…è¦ãªäº‹å®Ÿé …ç›®ã‚’åˆ—æŒ™ã€‚\n",
    "- æä¾›ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è©²å½“ç®‡æ‰€ã‚’æŠ½å‡ºã—ã€é‡è¤‡ãƒ»çŸ›ç›¾ãƒ»å‰å¾Œé–¢ä¿‚ã‚’æ•´ç†ã€‚\n",
    "- å¿…è¦æœ€å°é™ã®è¦ç´„ãƒ»è¨€ã„æ›ãˆã‚’è¡Œã„ã€æ—¥æœ¬èªã§ç°¡æ½”ã«è¨˜è¿°ã€‚\n",
    "- è©²å½“ãƒãƒ£ãƒ³ã‚¯ã® `id` ã‚’å¼•ç”¨ã¨ã—ã¦ä»˜ä¸ï¼ˆä¾‹: [doc:0053c7...]ï¼‰ã€‚\n",
    "- ååˆ†ãªæ ¹æ‹ ãŒãªã‘ã‚Œã°ã€ŒI don't knowã€ã€‚\n",
    "\n",
    "ã€ç¦æ­¢äº‹é …ã€‘\n",
    "- æ–­å®šã®ãŸã‚ã®è£œå®Œæ¨æ¸¬ã€å¹´ä»£/æ•°å€¤ã®å‰µä½œã€URLã‚„å›³è¡¨ã®å‰µä½œã€‚\n",
    "- JDocQAã«å«ã¾ã‚Œãªã„ç¯„å›²ã¸è©±ã‚’åºƒã’ã‚‹ã“ã¨ã€‚\n",
    "\n",
    "ã€å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹ã€‘\n",
    "- å˜æ–‡å›ç­”: ã€Œã€œã§ã™ã€‚[doc:abc123]ã€\n",
    "- ç®‡æ¡æ›¸ã: è¤‡æ•°æ¡ä»¶/æ‰‹é †ãŒã‚ã‚‹ã¨ãã®ã¿ä½¿ç”¨ã€‚\n",
    "- ä¸æ˜: ã€ŒI don't knowã€\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "instructions_formatted = \"\"\"\n",
    "You are a Japanese RAG answerer for JDocQA. Use ONLY the provided chunks.\n",
    "If insufficient evidence: reply with JSON whose \"answer\" is exactly \"I don't know\".\n",
    "\n",
    "Output MUST be a single JSON object:\n",
    "{\n",
    "  \"answer\": string,            // Japanese final answer or \"I don't know\"\n",
    "  \"citations\": [ { \"id\": string } ],  // chunk ids you used (at least one if answer != \"I don't know\")\n",
    "  \"confidence\": \"high\" | \"medium\" | \"low\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- No outside knowledge. No speculation.\n",
    "- Keep it brief and specific. If chunks conflict, note it briefly and choose the most specific source.\n",
    "- If you used multiple chunks, include all their ids in citations.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instructions}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_client = KnowledgeAgentRetrievalClient(\n",
    "    endpoint=SEARCH_ENDPOINT, agent_name=KNOWLEDGE_AGENT_NAME, credential=credential\n",
    ")\n",
    "query_1 = \"\"\"\n",
    "    å®¶æ—æ‰‹å½“ã®æ”¯çµ¦æ¡ä»¶ã¯ï¼Ÿ\n",
    "    \"\"\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": query_1})\n",
    "\n",
    "req = KnowledgeAgentRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeAgentMessage(\n",
    "            role=m[\"role\"],\n",
    "            content=[KnowledgeAgentMessageTextContent(text=m[\"content\"])],\n",
    "        )\n",
    "        for m in messages\n",
    "        if m[\"role\"] != \"system\"\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        SearchIndexKnowledgeSourceParams(\n",
    "            knowledge_source_name=KNOWLEDGE_SOURCE_NAME, kind=\"searchIndex\"\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = agent_client.retrieve(retrieval_request=req, api_version=SEARCH_API_VERSION)\n",
    "print(f\"Retrieved content from '{KNOWLEDGE_SOURCE_NAME}' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response\")\n",
    "print(textwrap.fill(result.response[0].content[0].text, width=120))\n",
    "\n",
    "print(\"Activity\")\n",
    "print(json.dumps([a.as_dict() for a in result.activity], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"Results\")\n",
    "print(\n",
    "    json.dumps([r.as_dict() for r in result.references], indent=2, ensure_ascii=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ai-search-agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
