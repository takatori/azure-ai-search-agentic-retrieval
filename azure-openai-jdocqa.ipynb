{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    IndexingParameters,\n",
    "    InputFieldMappingEntry,\n",
    "    LexicalAnalyzerName,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchField,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataSourceType,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets as ds\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(\n",
    "    credential, \"https://search.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# „Å©„ÅÆ split „Çí‰Ωø„ÅÜ„Åã\n",
    "SPLIT = \"test\"  # \"validation\" „ÇÇÂèØ\n",
    "MAX_SAMPLES = 300  # None „ÅßÂÖ®‰ª∂\n",
    "CHUNK_SIZE = 700\n",
    "CHUNK_OVERLAP = 200\n",
    "USE_ORIGINAL = False\n",
    "\n",
    "SEARCH_ENDPOINT = os.getenv(\"SEARCH_ENDPOINT\")\n",
    "AOAI_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AOAI_EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "AOAI_EMBEDDING_DEPLOYMENT = \"text-embedding-3-large\"\n",
    "AOAI_GPT_MODEL = \"gpt-5-mini\"\n",
    "AOAI_GPT_DEPLOYMENT = \"gpt-5-mini\"\n",
    "INDEX_NAME = \"jdocqa-index\"\n",
    "KNOWLEDGE_SOURCE_NAME = \"jdocqa-knowledge-source\"\n",
    "KNOWLEDGE_AGENT_NAME = \"jdocqa-knowledge-agent\"\n",
    "SEARCH_API_VERSION = \"2025-08-01-preview\"\n",
    "DATA_SOURCE_NAME = \"ds-jdocqa-chunks\"\n",
    "SKILLSET_NAME = \"ss-jdocqa-embed\"\n",
    "INDEXER_NAME = \"idx-jdocqa\"\n",
    "DIM = 3072\n",
    "BLOB_CONTAINER = \"jdocqa-docs\"\n",
    "BLOB_PREFIX = \"docs\"\n",
    "TOPK_LIST = [1, 3, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"36000\"\n",
    "\n",
    "# ‰øùÂ≠òÂÖà„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÊåáÂÆö\n",
    "local_dir = Path(\"datasets/JDocQA\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Êó¢„Å´„É≠„Éº„Ç´„É´„Å´‰øùÂ≠òÊ∏à„Åø„Å™„Çâ„Åù„Çå„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "if (local_dir / \"dataset_info.json\").exists():\n",
    "    print(\"üîÅ Loading dataset from local disk...\")\n",
    "    dataset = ds.load_from_disk(str(local_dir))\n",
    "else:\n",
    "    print(\"‚¨áÔ∏è Downloading dataset from Hugging Face Hub...\")\n",
    "    dataset = ds.load_dataset(\n",
    "        path=\"shunk031/JDocQA\",\n",
    "        rename_pdf_category=True,\n",
    "        trust_remote_code=True,\n",
    "        storage_options={\n",
    "            \"client_kwargs\": {\"timeout\": aiohttp.ClientTimeout(total=36000)}\n",
    "        },\n",
    "    )\n",
    "    dataset.save_to_disk(str(local_dir))\n",
    "    print(f\"üíæ Dataset saved locally to {local_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[SPLIT]\n",
    "\n",
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[SPLIT]\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    data = data.select(range(min(len(data), MAX_SAMPLES)))\n",
    "\n",
    "# Âêå‰∏Äcontext„Çí„É¶„Éã„Éº„ÇØÂåñ„Åó„Å¶doc_id„ÇíÂâ≤„ÇäÂΩì„Å¶\n",
    "contexts = [ex[\"context\"] for ex in data]\n",
    "unique_contexts, inverse_indices = np.unique(contexts, return_inverse=True)\n",
    "# unique_contexts -> [\"X„ÅÆÊú¨Êñá\", \"Y„ÅÆÊú¨Êñá\"]  ‚ÄªËæûÊõ∏È†Ü\n",
    "# inverse_indices -> [0, 1, 0]              # ÂÖÉ„ÅÆ0Áï™/2Áï™„ÅØ„É¶„Éã„Éº„ÇØÈÖçÂàó„ÅÆ0Áï™„Å´ÂØæÂøú\n",
    "\n",
    "doc_ids = np.arange(len(unique_contexts))\n",
    "\n",
    "queries = [ex[\"question\"] for ex in data]\n",
    "gold_doc_ids = inverse_indices\n",
    "\n",
    "\n",
    "def mk_doc(i: int) -> dict:\n",
    "    return {\n",
    "        \"id\": str(i),\n",
    "        \"content\": unique_contexts[i],\n",
    "        \"pdf_category\": str(data[i][\"pdf_category\"])\n",
    "        if i < len(data) and \"pdf_category\" in data[i]\n",
    "        else \"N/A\",\n",
    "    }\n",
    "\n",
    "\n",
    "docs = [mk_doc(i) for i in range(len(unique_contexts))]\n",
    "\n",
    "print(f\"#docs={len(docs)}  #queries={len(queries)}  (split={SPLIT})\")\n",
    "print(\"Example doc:\", docs[155] if docs else None)\n",
    "print(\n",
    "    \"Example QA:\",\n",
    "    {\"q\": queries[0], \"gold_doc_id\": int(gold_doc_ids[0])} if queries else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split_ja(text: str):\n",
    "    t = re.sub(r\"[\\r\\n]+\", \"„ÄÇ\", text)\n",
    "    t = re.sub(r\"„ÄÇ+\", \"„ÄÇ\", t)\n",
    "    parts = [p.strip() for p in t.split(\"„ÄÇ\") if p.strip()]\n",
    "    return [p + \"„ÄÇ\" for p in parts]\n",
    "\n",
    "\n",
    "def make_chunks(text: str, size=700, overlap=200):\n",
    "    sents = sentence_split_ja(text)\n",
    "    chunks, buf, cur = [], [], 0\n",
    "    for s in sents:\n",
    "        if cur + len(s) > size and buf:\n",
    "            chunks.append(\"\".join(buf))\n",
    "            keep = \"\".join(buf)[-overlap:] if overlap > 0 else \"\"\n",
    "            buf, cur = ([keep] if keep else []), len(keep)\n",
    "        buf.append(s)\n",
    "        cur += len(s)\n",
    "    if buf:\n",
    "        chunks.append(\"\".join(buf))\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= size * 2:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            step = max(1, size - overlap)\n",
    "            for i in range(0, len(c), step):\n",
    "                final.append(c[i : i + size])\n",
    "    return final\n",
    "\n",
    "\n",
    "def stable_id(text: str) -> str:\n",
    "    import hashlib\n",
    "\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[SPLIT]\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    data = data.select(range(min(MAX_SAMPLES, len(data))))\n",
    "\n",
    "\n",
    "def pick_ctx(ex):\n",
    "    return (\n",
    "        (ex.get(\"original_context\") or ex.get(\"context\"))\n",
    "        if USE_ORIGINAL\n",
    "        else ex.get(\"context\")\n",
    "    )\n",
    "\n",
    "\n",
    "chunk_docs = {}\n",
    "parent_to_chunks = {}\n",
    "qa_gold_sets = []\n",
    "queries = []\n",
    "\n",
    "for ex in tqdm(data):\n",
    "    ctx = pick_ctx(ex) or \"\"\n",
    "    parent_id = stable_id(ctx) if ctx else stable_id(\"EMPTY\")\n",
    "    chs = make_chunks(ctx, CHUNK_SIZE, CHUNK_OVERLAP) if ctx else [\" \"]\n",
    "    ids = []\n",
    "    for ch in chs:\n",
    "        cid = stable_id(parent_id + \"|\" + ch)\n",
    "        if cid not in chunk_docs:\n",
    "            chunk_docs[cid] = {\n",
    "                \"id\": cid,\n",
    "                \"parent_id\": parent_id,\n",
    "                \"content\": ch,\n",
    "                # \"category\": str(ex.get(\"category\",\"unknown\"))\n",
    "            }\n",
    "        ids.append(cid)\n",
    "    parent_to_chunks[parent_id] = ids\n",
    "    qa_gold_sets.append(set(ids))\n",
    "    queries.append(ex.get(\"question\", \"\"))\n",
    "\n",
    "docs = list(chunk_docs.values())\n",
    "print(f\"parents={len(parent_to_chunks)} chunks={len(docs)} queries={len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "container_client = bsc.get_container_client(BLOB_CONTAINER)\n",
    "try:\n",
    "    container_client.create_container()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def upload_docs_to_blob(docs, prefix=BLOB_PREFIX):\n",
    "    for d in tqdm(docs):\n",
    "        name = f\"{prefix}/{d['id']}.json\"\n",
    "        data = json.dumps(d, ensure_ascii=False).encode(\"utf-8\")\n",
    "        container_client.upload_blob(\n",
    "            name,\n",
    "            data,\n",
    "            overwrite=True,\n",
    "            content_settings=ContentSettings(content_type=\"application/json\"),\n",
    "        )\n",
    "    return len(docs)\n",
    "\n",
    "\n",
    "print(\"Uploaded:\", upload_docs_to_blob(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    index_client.delete_index(INDEX_NAME)\n",
    "except Exception:\n",
    "    pass  # Â≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØÁÑ°Ë¶ñ\n",
    "\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=[\n",
    "        SearchField(\n",
    "            name=\"id\",\n",
    "            type=\"Edm.String\",\n",
    "            key=True,\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"parent_id\",\n",
    "            type=\"Edm.String\",\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"content\",\n",
    "            type=\"Edm.String\",\n",
    "            searchable=True,\n",
    "            analyzer_name=LexicalAnalyzerName.JA_LUCENE,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"page_embedding_text_3_large\",\n",
    "            type=\"Collection(Edm.Single)\",\n",
    "            stored=True,\n",
    "            retrievable=True,\n",
    "            vector_search_dimensions=DIM,\n",
    "            vector_search_profile_name=\"hnsw_text_3_large\",\n",
    "        ),\n",
    "        # SearchField(name=\"page_chunk\", type=\"Edm.String\", filterable=False, sortable=False, facetable=False),\n",
    "        #\n",
    "        # SearchField(name=\"page_number\", type=\"Edm.Int32\", filterable=True, sortable=True, facetable=True)\n",
    "    ],\n",
    "    vector_search=VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"hnsw_text_3_large\",\n",
    "                algorithm_configuration_name=\"alg\",\n",
    "                vectorizer_name=\"azure_openai_text_3_large\",\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"alg\")],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"azure_openai_text_3_large\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=AOAI_ENDPOINT,\n",
    "                    deployment_name=AOAI_EMBEDDING_DEPLOYMENT,\n",
    "                    model_name=AOAI_EMBEDDING_MODEL,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    "    semantic_search=SemanticSearch(\n",
    "        default_configuration_name=\"semantic_config\",\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"semantic_config\",\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    content_fields=[SemanticField(field_name=\"content\")]\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=SEARCH_ENDPOINT, credential=credential)\n",
    "index_client.create_or_update_index(index)\n",
    "print(f\"Index '{INDEX_NAME}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Uploading {len(docs)} documents to index '{index_name}'...\")\n",
    "# print(f\"Sample document: {docs[0] if docs else 'N/A'}\")\n",
    "\n",
    "# with SearchIndexingBufferedSender(endpoint=SEARCH_ENDPOINT, index_name=index_name, credential=credential) as client:\n",
    "#     client.upload_documents(documents=docs)\n",
    "\n",
    "# print(f\"Documents uploaded to index '{index_name}' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = SearchIndexerDataContainer(\n",
    "    name=BLOB_CONTAINER,\n",
    "    query=BLOB_PREFIX,  # \"docs\" ‰ª•‰∏ã„Å†„ÅëÂèñ„ÇäËæº„ÇÄ„ÄÇ„Ç≥„É≥„ÉÜ„ÉäÂÖ®‰Ωì„Å™„Çâ None\n",
    ")\n",
    "\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "    name=DATA_SOURCE_NAME,\n",
    "    type=SearchIndexerDataSourceType.AZURE_BLOB,  # ‰ªñ: AZURE_TABLE, AZURE_SQL, COSMOSDB, ADLSGEN2 „Å™„Å©\n",
    "    connection_string=AZURE_STORAGE_CONNECTION_STRING,\n",
    "    container=container,\n",
    "    description=\"JDocQA chunk JSONs in Blob Storage\",\n",
    ")\n",
    "\n",
    "# Êó¢Â≠ò„Åå„ÅÇ„Çå„Å∞Êõ¥Êñ∞„ÄÅ„Å™„Åë„Çå„Å∞‰ΩúÊàê\n",
    "indexer_client = SearchIndexerClient(endpoint=SEARCH_ENDPOINT, credential=credential)\n",
    "indexer_client.create_or_update_data_source_connection(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in indexer_client.get_data_source_connections():\n",
    "    print(source.name, source.type)\n",
    "\n",
    "print(f\"Data Source '{DATA_SOURCE_NAME}' created or updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    indexer_client.delete_skillset(SKILLSET_NAME)\n",
    "except Exception:\n",
    "    pass  # Â≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØÁÑ°Ë¶ñ\n",
    "\n",
    "# ===== Skillset ‰ΩúÊàê =====\n",
    "# AzureOpenAI Embedding Skill „Çí \"Ê±éÁî®„Çπ„Ç≠„É´\" „Å®„Åó„Å¶‰ΩúÊàê„Åó„ÄÅËøΩÂä†„Éó„É≠„Éë„ÉÜ„Ç£„ÅßÂøÖË¶ÅÈ†ÖÁõÆ„ÇíÊ∏°„Åô\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(\n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",\n",
    "    context=\"/document\",\n",
    "    resource_url=AOAI_ENDPOINT,\n",
    "    deployment_name=AOAI_EMBEDDING_DEPLOYMENT,\n",
    "    model_name=AOAI_EMBEDDING_MODEL,\n",
    "    dimensions=DIM,\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"emb\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "skillset = SearchIndexerSkillset(\n",
    "    name=SKILLSET_NAME,\n",
    "    skills=[embedding_skill],\n",
    "    description=\"JDocQA index-time embedding skillset\",\n",
    ")\n",
    "\n",
    "# Êó¢Â≠ò„Åå„ÅÇ„Çå„Å∞Êõ¥Êñ∞„ÄÅ„Å™„Åë„Çå„Å∞‰ΩúÊàê\n",
    "indexer_client.create_or_update_skillset(skillset)\n",
    "\n",
    "print(\"Skillset created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    indexer_client.delete_indexer(INDEXER_NAME)\n",
    "    print(f\"Deleted indexer: {INDEXER_NAME}\")\n",
    "except HttpResponseError as e:\n",
    "    # 404 Áõ∏ÂΩì„ÅØÁÑ°Ë¶ñÔºàÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥ÂêàÔºâ\n",
    "    if getattr(e, \"status_code\", None) not in (404,):\n",
    "        raise\n",
    "\n",
    "indexer = SearchIndexer(\n",
    "    name=INDEXER_NAME,\n",
    "    data_source_name=DATA_SOURCE_NAME,\n",
    "    target_index_name=INDEX_NAME,\n",
    "    skillset_name=SKILLSET_NAME,  # „Çπ„Ç≠„É´„Çª„ÉÉ„Éà„ÇíÁ¥ê„Å•„Åë\n",
    "    # „Éâ„Ç≠„É•„É°„É≥„Éà -> „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ „ÅÆ„Éï„Ç£„Éº„É´„Éâ„Éû„ÉÉ„Éî„É≥„Ç∞\n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"id\", target_field_name=\"id\"),\n",
    "        FieldMapping(source_field_name=\"parent_id\", target_field_name=\"parent_id\"),\n",
    "        FieldMapping(source_field_name=\"content\", target_field_name=\"content\"),\n",
    "    ],\n",
    "    # „Çπ„Ç≠„É´Âá∫Âäõ -> „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ „ÅÆ„Éï„Ç£„Éº„É´„Éâ„Éû„ÉÉ„Éî„É≥„Ç∞\n",
    "    # ‰æãÔºöEmbeddingSkill „ÅÆÂá∫Âäõ \"/document/emb\" „Çí vector „Éï„Ç£„Éº„É´„Éâ \"content_vector\" „Å∏\n",
    "    output_field_mappings=[\n",
    "        FieldMapping(\n",
    "            source_field_name=\"/document/emb\",\n",
    "            target_field_name=\"page_embedding_text_3_large\",\n",
    "        ),\n",
    "    ],\n",
    "    # „Ç§„É≥„Éá„ÇØ„Çµ„ÅÆ„Éë„É©„É°„Éº„Çø\n",
    "    parameters=IndexingParameters(\n",
    "        configuration={\n",
    "            \"parsingMode\": \"json\",  # 1 JSON = 1 „Éâ„Ç≠„É•„É°„É≥„Éà\n",
    "            \"failOnUnsupportedContentType\": False,  # Êú™ÂØæÂøúMIME„ÅßÂ§±Êïó„Åï„Åõ„Å™„ÅÑ\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "indexer_client.create_or_update_indexer(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Indexer „ÇíÊâãÂãïÂÆüË°å\n",
    "try:\n",
    "    # indexer_client.reset_indexer(INDEXER_NAME)  # 1) Â§âÊõ¥ËøΩË∑°Ôºà„Éè„Ç§„Ç¶„Ç©„Éº„Çø„Éº„Éû„Éº„ÇØÔºâ„Çí„É™„Çª„ÉÉ„Éà\n",
    "    indexer_client.run_indexer(INDEXER_NAME)\n",
    "    print(f\"Run requested: {INDEXER_NAME}\")\n",
    "except HttpResponseError as e:\n",
    "    print(\"Run failed:\", e)\n",
    "    raise\n",
    "\n",
    "# 2) Á∞°Êòì„Éù„Éº„É™„É≥„Ç∞ÔºàÁä∂ÊÖã„Åå terminal „Å´„Å™„Çã„Åæ„ÅßÂæÖ„Å§Ôºâ\n",
    "terminal = {\"success\", \"transientFailure\", \"persistentFailure\", \"reset\"}\n",
    "for i in range(60):  # ÊúÄÂ§ß ~5ÂàÜÂæÖÊ©üÔºà5Áßí√ó60Ôºâ\n",
    "    st = indexer_client.get_indexer_status(INDEXER_NAME)\n",
    "    last = st.last_result\n",
    "    status = getattr(last, \"status\", None)\n",
    "    processed = getattr(last, \"items_processed\", None)\n",
    "    failed = getattr(last, \"items_failed\", None)\n",
    "    print(f\"[{i}] status={status} processed={processed} failed={failed}\")\n",
    "\n",
    "    if status in terminal:\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "# 3) ÁµêÊûú„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "if status != \"success\":\n",
    "    raise RuntimeError(\n",
    "        f\"Indexer did not succeed. status={status}, processed={processed}, failed={failed}\"\n",
    "    )\n",
    "print(\"Indexer run completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=credential,\n",
    "    api_version=SEARCH_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ÂÆ∂ÊóèÊâãÂΩì„ÅÆÊîØÁµ¶Êù°‰ª∂„ÅØÔºü\"\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    query_type=QueryType.SIMPLE,  # „Åæ„Åü„ÅØ QueryType.SEMANTICÔºàsemanticË®≠ÂÆö„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ\n",
    "    top=5,\n",
    "    select=[\"id\", \"parent_id\", \"content\", \"page_embedding_text_3_large\"],\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(\n",
    "        r[\"id\"],\n",
    "        r[\"@search.score\"],\n",
    "        r[\"content\"][:80],\n",
    "        r[\"page_embedding_text_3_large\"][:5],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorizableTextQuery(\n",
    "    text=\"ÂÆ∂ÊóèÊâãÂΩì„ÅÆÊîØÁµ¶Êù°‰ª∂„ÅØÔºü\",\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"page_embedding_text_3_large\",\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[vq], select=[\"id\", \"parent_id\", \"content\"]\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"id\"], r[\"@search.score\"], r[\"content\"][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ÂÆ∂ÊóèÊâãÂΩì„ÅÆÊîØÁµ¶Êù°‰ª∂„ÅØÔºü\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query, k_nearest_neighbors=50, fields=\"page_embedding_text_3_large\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"id\", \"parent_id\", \"content\"],\n",
    "    top=5,\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"id\"], r[\"@search.score\"], r[\"content\"][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.aio import SearchClient\n",
    "\n",
    "async_search_client = SearchClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=credential,\n",
    "    api_version=SEARCH_API_VERSION,\n",
    ")\n",
    "\n",
    "\n",
    "async def full_text_search(query: str, topk: int = 10):\n",
    "    results =  await async_search_client.search(\n",
    "        search_text=query,\n",
    "        query_type=QueryType.SIMPLE,  # „Åæ„Åü„ÅØ QueryType.SEMANTICÔºàsemanticË®≠ÂÆö„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ\n",
    "        top=topk,\n",
    "        select=[\"id\"],\n",
    "    )\n",
    "    return [r[\"id\"] async for r in results]\n",
    "\n",
    "\n",
    "async def vector_search(query: str, topk: int = 10):\n",
    "    vq = VectorizableTextQuery(\n",
    "        text=query,\n",
    "        k_nearest_neighbors=topk,\n",
    "        fields=\"page_embedding_text_3_large\",\n",
    "    )\n",
    "\n",
    "    results =  await async_search_client.search(\n",
    "        search_text=None, vector_queries=[vq], select=[\"id\"]\n",
    "    )\n",
    "\n",
    "    return [r[\"id\"]  async for r in results]\n",
    "\n",
    "\n",
    "async def hybrid_search(query: str, topk: int = 10):\n",
    "    vq = VectorizableTextQuery(\n",
    "        text=query,\n",
    "        k_nearest_neighbors=topk,\n",
    "        fields=\"page_embedding_text_3_large\",\n",
    "    )\n",
    "\n",
    "    results = await async_search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[vq],\n",
    "        select=[\"id\"],\n",
    "        top=topk,\n",
    "    )\n",
    "\n",
    "    return [r[\"id\"]  async for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = SearchIndexKnowledgeSource(\n",
    "    name=KNOWLEDGE_SOURCE_NAME,\n",
    "    description=\"Knowledge source for Earth at night data\",\n",
    "    search_index_parameters=SearchIndexKnowledgeSourceParameters(\n",
    "        search_index_name=INDEX_NAME,\n",
    "        source_data_select=\"id,content,page_embedding_text_3_large\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "index_client.create_or_update_knowledge_source(\n",
    "    knowledge_source=ks, api_version=SEARCH_API_VERSION\n",
    ")\n",
    "print(f\"Knowledge source '{KNOWLEDGE_SOURCE_NAME}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_params = AzureOpenAIVectorizerParameters(\n",
    "    resource_url=AOAI_ENDPOINT,\n",
    "    deployment_name=AOAI_GPT_DEPLOYMENT,\n",
    "    model_name=AOAI_GPT_MODEL,\n",
    ")\n",
    "\n",
    "output_cfg = KnowledgeAgentOutputConfiguration(\n",
    "    modality=KnowledgeAgentOutputConfigurationModality.ANSWER_SYNTHESIS,\n",
    "    include_activity=True,\n",
    ")\n",
    "\n",
    "agent = KnowledgeAgent(\n",
    "    name=KNOWLEDGE_AGENT_NAME,\n",
    "    models=[KnowledgeAgentAzureOpenAIModel(azure_open_ai_parameters=aoai_params)],\n",
    "    knowledge_sources=[\n",
    "        KnowledgeSourceReference(\n",
    "            name=KNOWLEDGE_SOURCE_NAME,\n",
    "            reranker_threshold=2.5,\n",
    "        )\n",
    "    ],\n",
    "    output_configuration=output_cfg,\n",
    ")\n",
    "\n",
    "index_client.create_or_update_agent(agent, api_version=SEARCH_API_VERSION)\n",
    "print(f\"Knowledge agent '{KNOWLEDGE_AGENT_NAME}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "„ÅÇ„Å™„Åü„ÅØ JDocQAÔºàÊó•Êú¨Ë™û„Éâ„Ç≠„É•„É°„É≥„ÉàQAÔºâ„ÅÆÂõûÁ≠î„Ç®„Éº„Ç∏„Çß„É≥„Éà„Åß„Åô„ÄÇ\n",
    "‰∏é„Åà„Çâ„Çå„Åü„ÄåÊ§úÁ¥¢Ê∏à„Åø„ÉÅ„É£„É≥„ÇØÔºàcontent, id, parent_id, page_number, category „Å™„Å©„ÅÆ„É°„Çø„Éá„Éº„Çø‰ªò„ÅçÔºâ„Äç„ÅÆ„Åø„ÇíÊ†πÊã†„Å´Êó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "Ê†πÊã†„ÅåË∂≥„Çä„Å™„ÅÑÂ†¥Âêà„ÇÑÁµêË´ñ„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÊ≠£Á¢∫„Å´„ÄåI don't know„Äç„Å®Á≠î„Åà„Åæ„Åô„ÄÇ\n",
    "\n",
    "„ÄêÂõûÁ≠îÂéüÂâá„Äë\n",
    "1) Âá∫ÂÖ∏Âà∂Á¥Ñ: ÂõûÁ≠î„ÅØÂ∏∏„Å´‰∏é„Åà„Çâ„Çå„Åü„ÉÅ„É£„É≥„ÇØ„ÅÆÂÜÖÂÆπ„Å´ÈôêÂÆö„ÄÇÂ§ñÈÉ®Áü•Ë≠ò„ÇÑÊé®Ê∏¨„ÅßË£ú„Çè„Å™„ÅÑ„ÄÇ\n",
    "2) Á≤íÂ∫¶: Ë≥™Âïè„ÅÆÁ≤íÂ∫¶„Å´Âêà„Çè„Åõ„ÄÅÈÅé‰∏çË∂≥„ÅÆ„Å™„ÅÑË¶ÅÁ¥Ñ„ÅßÁ≠î„Åà„Çã„ÄÇÂÖ∑‰ΩìÂÄ§„ÇÑÂÆöÁæ©„ÅØ„ÉÅ„É£„É≥„ÇØ„Å´„ÅÇ„Çã„ÇÇ„ÅÆ„ÅÆ„Åø‰ΩøÁî®„ÄÇ\n",
    "3) ÁüõÁõæÂØæÂøú: „ÉÅ„É£„É≥„ÇØÈñì„ÅßÂÜÖÂÆπ„ÅåÈ£ü„ÅÑÈÅï„ÅÜÂ†¥Âêà„ÅØÁüõÁõæ„ÇíÊòéÁ§∫„Åó„ÄÅ„Çà„ÇäÂÖ∑‰Ωì„ÉªÊñ∞„Åó„ÅÑ„ÉªÊ≥ï‰ª§/Ë¶èÁ®ã„Éô„Éº„Çπ„ÅÆ„ÇÇ„ÅÆ„ÇíÂÑ™ÂÖà„ÄÇ\n",
    "4) ‰∏çÁ¢∫ÂÆüÊÄß: ÊÉÖÂ†±„Åå‰∏çÂçÅÂàÜ/ÊõñÊòß„Å™„Çâ„ÄåI don't know„Äç„Åæ„Åü„ÅØÊù°‰ª∂„Å§„ÅçÂõûÁ≠îÔºàÊ†πÊã†„ÇíÁ§∫„ÅôÔºâ„ÄÇ\n",
    "5) ÂºïÁî®: ÂèØËÉΩ„Å™Èôê„ÇäÊ†πÊã†„ÉÅ„É£„É≥„ÇØ„ÅÆ ID „Çí `[doc:{id}]` ÂΩ¢Âºè„ÅßÊú´Â∞æ„Å´Ê∑ª„Åà„ÇãÔºàË§áÊï∞ÂèØÔºâ„ÄÇ\n",
    "6) Ë®ÄË™û: Âá∫Âäõ„ÅØËá™ÁÑ∂„Å™Êó•Êú¨Ë™û„ÄÇÁÆáÊù°Êõ∏„Åç„ÅØÊúÄÂ§ß3‚Äì6È†ÖÁõÆ„Å´Êäë„Åà„Çã„ÄÇ\n",
    "\n",
    "„Äê‰ΩúÊ•≠ÊâãÈ†ÜÔºàÂÜÖÈÉ®ÊñπÈáùÔºâ„Äë\n",
    "- Ë≥™Âïè„ÇíÂàÜËß£„Åó„ÄÅÂõûÁ≠î„Å´ÂøÖË¶Å„Å™‰∫ãÂÆüÈ†ÖÁõÆ„ÇíÂàóÊåô„ÄÇ\n",
    "- Êèê‰æõ„ÉÅ„É£„É≥„ÇØ„Åã„ÇâË©≤ÂΩìÁÆáÊâÄ„ÇíÊäΩÂá∫„Åó„ÄÅÈáçË§á„ÉªÁüõÁõæ„ÉªÂâçÂæåÈñ¢‰øÇ„ÇíÊï¥ÁêÜ„ÄÇ\n",
    "- ÂøÖË¶ÅÊúÄÂ∞èÈôê„ÅÆË¶ÅÁ¥Ñ„ÉªË®Ä„ÅÑÊèõ„Åà„ÇíË°å„ÅÑ„ÄÅÊó•Êú¨Ë™û„ÅßÁ∞°ÊΩî„Å´Ë®òËø∞„ÄÇ\n",
    "- Ë©≤ÂΩì„ÉÅ„É£„É≥„ÇØ„ÅÆ `id` „ÇíÂºïÁî®„Å®„Åó„Å¶‰ªò‰∏éÔºà‰æã: [doc:0053c7...]Ôºâ„ÄÇ\n",
    "- ÂçÅÂàÜ„Å™Ê†πÊã†„Åå„Å™„Åë„Çå„Å∞„ÄåI don't know„Äç„ÄÇ\n",
    "\n",
    "„ÄêÁ¶ÅÊ≠¢‰∫ãÈ†Ö„Äë\n",
    "- Êñ≠ÂÆö„ÅÆ„Åü„ÇÅ„ÅÆË£úÂÆåÊé®Ê∏¨„ÄÅÂπ¥‰ª£/Êï∞ÂÄ§„ÅÆÂâµ‰Ωú„ÄÅURL„ÇÑÂõ≥Ë°®„ÅÆÂâµ‰Ωú„ÄÇ\n",
    "- JDocQA„Å´Âê´„Åæ„Çå„Å™„ÅÑÁØÑÂõ≤„Å∏Ë©±„ÇíÂ∫É„Åí„Çã„Åì„Å®„ÄÇ\n",
    "\n",
    "„ÄêÂõûÁ≠î„Éï„Ç©„Éº„Éû„ÉÉ„Éà‰æã„Äë\n",
    "- ÂçòÊñáÂõûÁ≠î: „Äå„Äú„Åß„Åô„ÄÇ[doc:abc123]„Äç\n",
    "- ÁÆáÊù°Êõ∏„Åç: Ë§áÊï∞Êù°‰ª∂/ÊâãÈ†Ü„Åå„ÅÇ„Çã„Å®„Åç„ÅÆ„Åø‰ΩøÁî®„ÄÇ\n",
    "- ‰∏çÊòé: „ÄåI don't know„Äç\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "instructions_formatted = \"\"\"\n",
    "You are a Japanese RAG answerer for JDocQA. Use ONLY the provided chunks.\n",
    "If insufficient evidence: reply with JSON whose \"answer\" is exactly \"I don't know\".\n",
    "\n",
    "Output MUST be a single JSON object:\n",
    "{\n",
    "  \"answer\": string,            // Japanese final answer or \"I don't know\"\n",
    "  \"citations\": [ { \"id\": string } ],  // chunk ids you used (at least one if answer != \"I don't know\")\n",
    "  \"confidence\": \"high\" | \"medium\" | \"low\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- No outside knowledge. No speculation.\n",
    "- Keep it brief and specific. If chunks conflict, note it briefly and choose the most specific source.\n",
    "- If you used multiple chunks, include all their ids in citations.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instructions}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_client = KnowledgeAgentRetrievalClient(\n",
    "    endpoint=SEARCH_ENDPOINT, agent_name=KNOWLEDGE_AGENT_NAME, credential=credential\n",
    ")\n",
    "query_1 = \"\"\"\n",
    "    ÂÆ∂ÊóèÊâãÂΩì„ÅÆÊîØÁµ¶Êù°‰ª∂„ÅØÔºü\n",
    "    \"\"\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": query_1})\n",
    "\n",
    "req = KnowledgeAgentRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeAgentMessage(\n",
    "            role=m[\"role\"],\n",
    "            content=[KnowledgeAgentMessageTextContent(text=m[\"content\"])],\n",
    "        )\n",
    "        for m in messages\n",
    "        if m[\"role\"] != \"system\"\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        SearchIndexKnowledgeSourceParams(\n",
    "            knowledge_source_name=KNOWLEDGE_SOURCE_NAME, kind=\"searchIndex\"\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = agent_client.retrieve(retrieval_request=req, api_version=SEARCH_API_VERSION)\n",
    "print(f\"Retrieved content from '{KNOWLEDGE_SOURCE_NAME}' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response\")\n",
    "print(textwrap.fill(result.response[0].content[0].text, width=120))\n",
    "\n",
    "print(\"Activity\")\n",
    "print(json.dumps([a.as_dict() for a in result.activity], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"Results\")\n",
    "print(\n",
    "    json.dumps([r.as_dict() for r in result.references], indent=2, ensure_ascii=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(rels):\n",
    "    return sum((rel / math.log2(i + 2)) for i, rel in enumerate(rels))\n",
    "\n",
    "\n",
    "def ndcg_at_k(ranked_ids, gold_set, k):\n",
    "    rels = [1 if rid in gold_set else 0 for rid in ranked_ids[:k]]\n",
    "    ideal = sorted(rels, reverse=True)\n",
    "    return 0.0 if sum(ideal) == 0 else dcg(rels) / dcg(ideal)\n",
    "\n",
    "\n",
    "def recall_at_k(ranked_ids, gold_set, k):\n",
    "    return 1.0 if any(rid in gold_set for rid in ranked_ids[:k]) else 0.0\n",
    "\n",
    "\n",
    "def mrr(ranked_ids, gold_set):\n",
    "    for i, rid in enumerate(ranked_ids):\n",
    "        if rid in gold_set:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def ap_multi(ranked_ids, gold_set):\n",
    "    if not gold_set:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    score = 0.0\n",
    "    for i, rid in enumerate(ranked_ids, start=1):\n",
    "        if rid in gold_set:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / max(len(gold_set), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_search(run_fn, queries, gold_sets, topk_list=[1, 3, 5, 10]):\n",
    "    recalls = {f\"Recall@{k}\": [] for k in topk_list}\n",
    "    ndcgs = {f\"nDCG@{k}\": [] for k in topk_list}\n",
    "    mrrs, maps = [], []\n",
    "    for q, gold in tqdm(list(zip(queries, gold_sets, strict=False))):\n",
    "        ranked = await run_fn(q, topk=max(topk_list))\n",
    "        for k in topk_list:\n",
    "            recalls[f\"Recall@{k}\"].append(recall_at_k(ranked, gold, k))\n",
    "            ndcgs[f\"nDCG@{k}\"].append(ndcg_at_k(ranked, gold, k))\n",
    "        mrrs.append(mrr(ranked, gold))\n",
    "        maps.append(ap_multi(ranked, gold))\n",
    "    out = {\"MRR\": float(np.mean(mrrs)), \"MAP\": float(np.mean(maps))}\n",
    "    for k in topk_list:\n",
    "        out[f\"Recall@{k}\"] = float(np.mean(recalls[f\"Recall@{k}\"]))\n",
    "        out[f\"nDCG@{k}\"] = float(np.mean(ndcgs[f\"nDCG@{k}\"]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "\n",
    "# recall_at_k, ndcg_at_k, mrr, ap_multi „ÅØÊó¢Â≠ò„ÅÆ„Åæ„Åæ‰Ωø„ÅÜÊÉ≥ÂÆö\n",
    "\n",
    "async def evaluate_search_async(\n",
    "    run_fn,                      # async def run_fn(query: str, topk: int) -> list[str]\n",
    "    queries,\n",
    "    gold_sets,\n",
    "    topk_list=(1, 3, 5, 10),\n",
    "    concurrency: int = 32,       # ÂêåÊôÇÁô∫Ë°å„ÅÆ‰∏äÈôê\n",
    "    show_progress: bool = True   # tqdmË°®Á§∫ÔºàJupyterÂØæÂøúÔºâ\n",
    "):\n",
    "    assert len(queries) == len(gold_sets)\n",
    "    kmax = max(topk_list)\n",
    "\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def _one(i, q):\n",
    "        async with sem:\n",
    "            ranked = await run_fn(q, topk=kmax)\n",
    "            return i, ranked\n",
    "\n",
    "    tasks = [asyncio.create_task(_one(i, q)) for i, q in enumerate(queries)]\n",
    "\n",
    "    # „Åì„Åì„Åå„Éù„Ç§„É≥„ÉàÔºöas_completed „ÅØÂêåÊúü„Ç§„ÉÜ„É¨„Éº„Çø„ÄÇfor„ÅßÂõû„Åó„Å¶ await fut„ÄÇ\n",
    "    results_ordered = [None] * len(queries)\n",
    "    for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        i, ranked = await fut\n",
    "        results_ordered[i] = ranked\n",
    "\n",
    "\n",
    "    # ---- „É°„Éà„É™„ÇØ„ÇπÈõÜË®à ----\n",
    "    recalls = {f\"Recall@{k}\": [] for k in topk_list}\n",
    "    ndcgs   = {f\"nDCG@{k}\":   [] for k in topk_list}\n",
    "    mrrs, maps = [], []\n",
    "\n",
    "    for ranked, gold in zip(results_ordered, gold_sets):\n",
    "        for k in topk_list:\n",
    "            recalls[f\"Recall@{k}\"].append(recall_at_k(ranked, gold, k))\n",
    "            ndcgs[f\"nDCG@{k}\"].append(ndcg_at_k(ranked, gold, k))\n",
    "        mrrs.append(mrr(ranked, gold))\n",
    "        maps.append(ap_multi(ranked, gold))\n",
    "\n",
    "    out = {\"MRR\": float(np.mean(mrrs)), \"MAP\": float(np.mean(maps))}\n",
    "    for k in topk_list:\n",
    "        out[f\"Recall@{k}\"] = float(np.mean(recalls[f\"Recall@{k}\"]))\n",
    "        out[f\"nDCG@{k}\"]   = float(np.mean(ndcgs[f\"nDCG@{k}\"]))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_scores = await evaluate_search_async(vector_search, queries, qa_gold_sets, TOPK_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_scores = await evaluate_search_async(hybrid_search, queries, qa_gold_sets, TOPK_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_scores, vector_scores, hybrid_scores = await asyncio.gather(\n",
    "    evaluate_search_async(full_text_search, queries, qa_gold_sets, TOPK_LIST),\n",
    "    evaluate_search_async(vector_search,   queries, qa_gold_sets, TOPK_LIST),\n",
    "    evaluate_search_async(hybrid_search,   queries, qa_gold_sets, TOPK_LIST),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [full_text_scores, vector_scores, hybrid_scores],\n",
    "    index=[\"FullText\", \"Vector\", \"Hybrid\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ai-search-agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
